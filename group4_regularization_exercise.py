# -*- coding: utf-8 -*-
"""Group4_Regularization - Exercise.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mDLRx8Z6vhhjIr47Z7bu54aDCGHJtwrW

# Excercise - Creating our own custom Model

This is a notebook that provides a quick overview of how to create your own custom model. You will be creating a simple model.
You will be utilizing Keras and Tensorflow

## Water Quality Dataset

This dataset contains water quality measurements and assessments related to potability, which is the suitability of water for human consumption. The dataset's primary objective is to provide insights into water quality parameters and assist in determining whether the water is potable or not. Each row in the dataset represents a water sample with specific attributes, and the "Potability" column indicates whether the water is suitable for consumption.

https://www.kaggle.com/datasets/uom190346a/water-quality-and-potability?select=water_potability.csv
"""

pip install tensorflow

import tensorflow as tf

from sklearn.datasets import make_moons
from sklearn.preprocessing import StandardScaler
from matplotlib import pyplot
import pandas as pd

path = 'water_potability.csv'

df = pd.read_csv(path)

df.head()

"""Plot the Data Appropriately

# 1- Data preprocessing

## Here we impute the missing values in the data by changing them with mean and then we normalize the data to the same scale (feature scaling) and at the end we split it into training set and test set
"""

# generate 2d classification dataset
X, y = make_moons(n_samples=100, noise=0.2, random_state=1)

#Splitting the dataset into target and features
target = "Potability"
X = df.drop(target, axis=1)
Y = df[target]

X.shape

#Filling missing values with the mean
X.fillna(X.mean(), inplace=True)

X.info()

#Scaling the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled

"""# 2- Data splitting

## Here we splitted the data into training and test set using sckit learn
"""

#from sklearn import train_test_split
from sklearn.model_selection import train_test_split
#trainX, testX = X[:n_train, :], X[n_train:, :]

trainX, testX, trainY, testY = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)
# split into train and test

# n_train =  len(X_scaled/80)*100
# trainX, testX = X_scaled[n_train:, :], X_scaled[:n_train, :]
# trainY, testY = Y[n_train:], Y[:n_train]

print(trainX.shape)
print(testX.shape)
print(trainY.shape)
print(testY.shape)

"""# 3- Model architecture

We are settling an initial model architecture which may change later if we are facing  difficulties getting fair accuracies
"""

trainX.shape[1]

from keras.models import Sequential
from keras.layers import Dense
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(trainX.shape[1],)))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.summary()

"""# Start the training Process

1. **Using adam optimizer on early stopping**
"""

from keras.callbacks import EarlyStopping
from keras.optimizers import Adam

es = EarlyStopping()

#Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# fit model
history = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=4000, verbose=0, callbacks=[es])


# evaluate the model
_, train_acc = model.evaluate(trainX, trainY, verbose=0)
_, test_acc = model.evaluate(testX, testY, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))


# plot training history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()

model.summary()

print(testY)

import numpy as np
single_input = testX[2]
single_input = np.expand_dims(single_input, axis=0)

prediction = model.predict(single_input)

# Convert the prediction to binary output
binary_output = (prediction > 0.5).astype(int)

print(f'Prediction (Probability): {prediction[0][0]}')
print(f'Binary Output: {binary_output[0][0]}')

model.save('model1.keras')

"""**2. Comparing how it performs without using early stopping**"""

# fit model
history = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=200, verbose=1)#callbacks=[es])
# evaluate the model
_, train_acc = model.evaluate(trainX, trainY, verbose=0)
_, test_acc = model.evaluate(testX, testY, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
# plot training history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()

"""## Trying to predict using the new model"""

print(testY)

import numpy as np
single_input = testX[5]
single_input = np.expand_dims(single_input, axis=0)
print(single_input)

prediction = model.predict(single_input)

# Convert the prediction to binary output
binary_output = (prediction > 0.5).astype(int)

print(f'Prediction (Probability): {prediction[0][0]}')
print(f'Binary Output: {binary_output[0][0]}')

model.save('model2.keras')

"""**3. Using adam optimizer on L1**"""

from keras.regularizers import l1


model_l1 = Sequential()
model_l1.add(Dense(64, activation='relu', input_shape=(trainX.shape[1],), kernel_regularizer=l1(0.001)))
model_l1.add(Dense(32, activation='relu', kernel_regularizer=l1(0.001)))
model_l1.add(Dense(1, activation='sigmoid', kernel_regularizer=l1(0.001)))

#Compile the model
model_l1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# fit model
history = model_l1.fit(trainX, trainY, validation_data=(testX, testY), epochs=4000, verbose=0, callbacks=[es])


# evaluate the model
_, train_acc = model_l1.evaluate(trainX, trainY, verbose=0)
_, test_acc = model_l1.evaluate(testX, testY, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))


# plot training history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()

model.save('model_l1.keras')

"""**4. Using adam optimizer on L2**"""

from keras.regularizers import l2


model_l2 = Sequential()
model_l2.add(Dense(64, activation='relu', input_shape=(trainX.shape[1],), kernel_regularizer=l2(0.001)))
model_l2.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))
model_l2.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001)))

#Compile the model
model_l2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# fit model
history = model_l2.fit(trainX, trainY, validation_data=(testX, testY), epochs=4000, verbose=0, callbacks=[es])


# evaluate the model
_, train_acc = model_l2.evaluate(trainX, trainY, verbose=0)
_, test_acc = model_l2.evaluate(testX, testY, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))


# plot training history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()

model.save('model_l2.keras')

"""# Testing it out"""

print(testY)

import numpy as np
single_input = testX[1]
single_input = np.expand_dims(single_input, axis=0)

prediction = model_l2.predict(single_input)

# Convert the prediction to binary output
binary_output = (prediction > 0.5).astype(int)

print(f'Prediction (Probability): {prediction[0][0]}')
print(f'Binary Output: {binary_output[0][0]}')

"""Compare the output without Early stopping

**5. Using SGD optimizer on L1**
"""

from keras.callbacks import EarlyStopping
from keras.optimizers import SGD

es = EarlyStopping()

#Compile the model
model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])

# fit model
history = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=4000, verbose=0, callbacks=[es])


# evaluate the model
_, train_acc = model.evaluate(trainX, trainY, verbose=0)
_, test_acc = model.evaluate(testX, testY, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))


# plot training history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()

"""# Model Evaluation


"""

loss, accu = model.evaluate(X, Y)
print(f'Test loss: {loss:.3f}')
print(f'Test accuracy: {accu:.3f}')

!pip install fastapi uvicorn

import uvicorn
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
import numpy as np
import multiprocessing as mp

app = FastAPI()

# Load the TensorFlow model
model = tf.keras.models.load_model('model_l1.keras')

# Load the scaler and fit it to your data
scaler = StandardScaler()
# Assuming you have your data stored in a numpy array called 'X'
# You may load your dataset here and fit the scaler
# scaler.fit(X)

class InputData(BaseModel):
    data: List[List[float]]

@app.get("/greet/{name}")
async def greet_user(name: str):
    return {"message": f"Hello, {name}!"}

@app.post("/predict/")
async def predict(data: InputData):
    # Convert data to numpy array
    input_data = np.array(data.data)

    # Transform the input data using the fitted scaler
    input_data_scaled = scaler.transform(input_data)

    # Make predictions
    predictions = model.predict(input_data_scaled)

    return {"predictions": predictions.tolist()}

import asyncio
import uvicorn

async def main():
    uvicorn.run(app, host="127.0.0.1", port=8000)

if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    task = loop.create_task(main())
    try:
        loop.run_forever()
    except KeyboardInterrupt:
        task.cancel()
        loop.run_until_complete(asyncio.gather(task, return_exceptions=True))
    finally:
        loop.close()

import uvicorn
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
async def read_root():
    return {"Hello": "World"}

if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8000)

